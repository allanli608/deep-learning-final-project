{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20496bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import gc\n",
    "from src.final_models import PivotBaseline, ZeroShotNeutralizer, SyntheticNeutralizer\n",
    "from src.evaluate import Evaluator\n",
    "\n",
    "# 1. Load the Gold Test Data\n",
    "# Ensure you ran 'src/data_collection/create_pseudo_gold.py' first!\n",
    "test_df = pd.read_csv(\"data/processed/test_chinese_gold.csv\")\n",
    "inputs = test_df[\"Chinese_Biased\"].tolist()\n",
    "gold_refs = test_df[\"Chinese_Neutral_Gold\"].tolist()\n",
    "\n",
    "print(f\"Loaded {len(inputs)} test cases.\")\n",
    "\n",
    "# Dictionary to store outputs from each model\n",
    "model_outputs = {}\n",
    "\n",
    "# Define the models we want to test\n",
    "# We use a list of classes so we can instantiate -> run -> delete to save RAM\n",
    "model_classes = [\n",
    "    (\"Baseline (Pivot)\", PivotBaseline),\n",
    "    (\"Model 2 (ZeroShot)\", ZeroShotNeutralizer),\n",
    "    (\"Model 3 (Synthetic)\", SyntheticNeutralizer),\n",
    "]\n",
    "\n",
    "for name, ModelClass in model_classes:\n",
    "    print(f\"\\nüöÄ Running Inference for: {name}...\")\n",
    "\n",
    "    try:\n",
    "        # A. Initialize Model\n",
    "        model_instance = ModelClass()\n",
    "\n",
    "        # B. Generate\n",
    "        # Using batch_debias if available, else loop\n",
    "        outputs = [model_instance.debias(text) for text in inputs]\n",
    "        model_outputs[name] = outputs\n",
    "\n",
    "        # C. Cleanup (CRITICAL for GPU Memory)\n",
    "        del model_instance\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to run {name}: {e}\")\n",
    "        model_outputs[name] = [\"Error\"] * len(inputs)\n",
    "\n",
    "print(\"\\n‚úÖ Inference Complete. Starting Evaluation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize Evaluator (Loads BERT Judge + GPT2)\n",
    "evaluator = Evaluator()\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for model_name, predictions in model_outputs.items():\n",
    "    print(f\"\\nüìä Evaluating {model_name}...\")\n",
    "\n",
    "    # Check for empty/error outputs\n",
    "    if not predictions or predictions[0] == \"Error\":\n",
    "        continue\n",
    "\n",
    "    # A. Style Accuracy (Did it remove bias?)\n",
    "    acc = evaluator.get_style_accuracy(predictions)\n",
    "\n",
    "    # B. Content Preservation (Did it keep meaning?)\n",
    "    # Compare Prediction vs Original Biased Input (or Gold Neutral if you prefer)\n",
    "    # Comparing to Input checks if we kept the *topic*.\n",
    "    # Comparing to Gold checks if we matched the *reference*.\n",
    "    # Standard Style Transfer usually compares to INPUT for preservation.\n",
    "    bert_score = evaluator.get_bert_score(predictions, inputs)\n",
    "\n",
    "    # C. Fluency (Is it natural Chinese?)\n",
    "    ppl = evaluator.get_perplexity(predictions)\n",
    "\n",
    "    # D. Composite Score (Geometric Mean)\n",
    "    # We use 100/PPL scaling just to make the number readable in the table (e.g., 0.45 instead of 0.004)\n",
    "    # Formula: (Acc * BERT * (100/PPL))^(1/3)\n",
    "    composite = evaluator.calculate_composite_score(acc, bert_score, ppl)\n",
    "\n",
    "    results_table.append(\n",
    "        {\n",
    "            \"Model\": model_name,\n",
    "            \"Style Acc (‚Üë)\": f\"{acc:.2%}\",\n",
    "            \"Content Sim (‚Üë)\": f\"{bert_score:.3f}\",\n",
    "            \"Fluency PPL (‚Üì)\": f\"{ppl:.2f}\",\n",
    "            \"Composite Score\": f\"{composite:.3f}\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b25a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PART 3: FINAL REPORT\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\" FINAL RESEARCH RESULTS \")\n",
    "print(\"=\" * 40)\n",
    "df_results = pd.DataFrame(results_table)\n",
    "display(df_results)\n",
    "\n",
    "# Save for your paper\n",
    "df_results.to_csv(\"results/final_metrics_table.csv\", index=False)\n",
    "\n",
    "# Optional: Show a few qualitative examples\n",
    "print(\"\\n--- Qualitative Analysis (First 3 Examples) ---\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nInput: {inputs[i]}\")\n",
    "    for model_name, preds in model_outputs.items():\n",
    "        print(f\"[{model_name}]: {preds[i]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
