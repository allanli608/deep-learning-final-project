{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b82da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable hot-reloading so if you edit src/train.py, it updates here immediately\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from src.model import MBartNeutralizer\n",
    "from src.train import WNCDataset, WeightedSeq2SeqTrainer\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b40f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the architecture\n",
    "neutralizer = MBartNeutralizer(model_name=\"facebook/mbart-large-50\")\n",
    "model = neutralizer.get_model()\n",
    "tokenizer = neutralizer.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the filtered \"Complex\" dataset created by preprocess.py\n",
    "train_set = WNCDataset(\"data/processed/train_complex.csv\", tokenizer)\n",
    "val_set = WNCDataset(\"data/processed/val_complex.csv\", tokenizer)\n",
    "\n",
    "# Sanity Check: Print one example to show weights are working\n",
    "sample = train_set[0]\n",
    "print(\"Input Shape:\", sample[\"input_ids\"].shape)\n",
    "print(\"Weight Mask Sum:\", sample[\"loss_weights\"].sum())  # Should be < 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,  # Adjust based on your GPU VRAM\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-5,  # Lower LR for fine-tuning\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    fp16=True,  # Essential for mBART memory efficiency\n",
    "    remove_unused_columns=False,  # IMPORTANT: Keep 'loss_weights' in the batch\n",
    ")\n",
    "\n",
    "trainer = WeightedSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will output the live loss curve\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned weights\n",
    "neutralizer.save_model(\"models/mbart_neutralizer_en_v1\")\n",
    "\n",
    "# Quick Inference Test\n",
    "input_text = \"The radical regime failed to act.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs)\n",
    "print(\"Output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
