{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30d49e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026b82da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "# Enable hot-reloading so if you edit src/train.py, it updates here immediately\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from src import MBartNeutralizer, WNCDataset, WeightedSeq2SeqTrainer\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6b40f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing mBART on cuda...\n"
     ]
    }
   ],
   "source": [
    "neutralizer = MBartNeutralizer(model_name=\"facebook/mbart-large-50\")\n",
    "model = neutralizer.get_model()\n",
    "tokenizer = neutralizer.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d07f87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([128])\n",
      "Labels Shape: torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the filtered \"Complex\" dataset created by preprocess.py\n",
    "train_set = WNCDataset(\"data/processed/train_complex.csv\", tokenizer)\n",
    "val_set = WNCDataset(\"data/processed/val_complex.csv\", tokenizer)\n",
    "\n",
    "# Sanity Check: Print one example\n",
    "sample = train_set[0]\n",
    "print(\"Input Shape:\", sample[\"input_ids\"].shape)\n",
    "print(\"Labels Shape:\", sample[\"labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057c24fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12923/1370919502.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedSeq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,  # Adjust based on your GPU VRAM\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,  # Lower LR for fine-tuning\n",
    "    logging_steps=100,\n",
    "    save_steps=1500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1500,\n",
    "    fp16=True,  # Essential for mBART memory efficiency\n",
    "    remove_unused_columns=False,  # IMPORTANT: Keep 'loss_weights' in the batch\n",
    ")\n",
    "\n",
    "trainer = WeightedSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12923' max='16968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12923/16968 1:45:12 < 32:56, 2.05 it/s, Epoch 2.28/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>0.353512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.349500</td>\n",
       "      <td>0.339190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>0.328251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.257600</td>\n",
       "      <td>0.328729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.327250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.322399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.322384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.331449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This will output the live loss curve\n",
    "# train_result = trainer.train()\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b5368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/mbart_neutralizer_en_v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/deeplearning-gpu/lib/python3.10/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\n",
      "Caching is incompatible with gradient checkpointing in MBartDecoderLayer. Setting `past_key_values=None`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: it of ( ) 1. # the - course failed failed failed failed attempt attempt attempt attempt attempt at at the the\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned weights\n",
    "neutralizer.save_model(\"models/mbart_neutralizer_en_v1\")\n",
    "\n",
    "# Quick Inference Test\n",
    "input_text = \"The radical regime failed to act.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs)\n",
    "print(\"Output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7f19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./results/checkpoint-500...\n",
      "Initializing mBART on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './results/checkpoint-500' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 500 CHECKPOINT RESULTS ---\n",
      "Input:  The controversial leader foolishly denied the request.\n",
      "Output: the controversial leader denied the request.\n",
      "------------------------------\n",
      "Input:  The radical regime failed to provide for its citizens.\n",
      "Output: the regime failed to provide for its citizens.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading fine-tuned model for final verification...\")\n",
    "# Make sure this matches the path you saved to\n",
    "saved_path = \"models/mbart_neutralizer_en_v1\" \n",
    "neutralizer = MBartNeutralizer(model_name=saved_path)\n",
    "model = neutralizer.get_model()\n",
    "tokenizer = neutralizer.get_tokenizer()\n",
    "\n",
    "# 2. Define \"The Gauntlet\" (Test Cases)\n",
    "test_cases = [\n",
    "    # Case 1: Subjective Intensifier (Easy)\n",
    "    \"The radical regime failed to act on the crisis.\",\n",
    "    \n",
    "    # Case 2: Framing Bias (Harder - subtle verb change)\n",
    "    \"The controversial politician foolishly denied the allegations.\",\n",
    "    \n",
    "    # Case 3: Presupposition (Hardest - implies guilt)\n",
    "    \"He exposed the senator's corruption.\" \n",
    "]\n",
    "\n",
    "# 3. Run Robust Inference\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\n{'='*20} PHASE 1 COMPLETE: ENGLISH BASELINE {'='*20}\\n\")\n",
    "\n",
    "for text in test_cases:\n",
    "    # A. Tokenize (Force English Source)\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # B. Generate (Prevent Repetition & Force English Output)\n",
    "    generated_ids = model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"], \n",
    "        max_length=64,\n",
    "        num_beams=5,             # Smarter search\n",
    "        no_repeat_ngram_size=2,  # Prevents \"same same\" loops\n",
    "        repetition_penalty=1.2,  # Soft penalty to encourage natural phrasing\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # C. Decode\n",
    "    output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # D. Display\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Neutral:  {output}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nIf the 'Neutral' outputs removed the biased words (radical, foolishly, exposed)\")\n",
    "print(\"while keeping the facts, Phase 1 is SUCCESSFUL.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
